{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNSysNHnJc5eM/cgZYl7XDQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banjodayo39/deep_learning_project/blob/master/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwE5h0S8Xwdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding, \\\n",
        "  Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxmfGvRmYVUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58323de1-e720-4956-8f22-502e46c6f623"
      },
      "source": [
        "\n",
        "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
        "  from keras.layers import CuDNNLSTM as LSTM\n",
        "  from keras.layers import CuDNNGRU as GRU\n",
        "  print(\"Yes\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvjLIEwBX3_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure we do softmax over the time axis\n",
        "# expected shape is N x T x D\n",
        "# note: the latest version of Keras allows you to pass in axis arg\n",
        "def softmax_over_time(x):\n",
        "  assert(K.ndim(x) > 2)\n",
        "  e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
        "  s = K.sum(e, axis=1, keepdims=True)\n",
        "  return e / s\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktGs0RNPYFY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# config\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "LATENT_DIM = 256\n",
        "LATENT_DIM_DECODER = 256 # idea: make it different to ensure things all fit together properly!\n",
        "NUM_SAMPLES = 10000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6g_sLdPYJ9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Where we will store the data\n",
        "input_texts = [] # sentence in original language\n",
        "target_texts = [] # sentence in target language\n",
        "target_texts_inputs = [] # sentence in target language offset by 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfBBjDKpYMw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "8813e2d7-565a-47c8-b6dc-0644a2ce09c3"
      },
      "source": [
        "# load in the data\n",
        "# download the data at: http://www.manythings.org/anki/\n",
        "t = 0\n",
        "for line in open('sample_data/fra.txt'):\n",
        "  # only keep a limited number of samples\n",
        "  t += 1\n",
        "  if t > NUM_SAMPLES:\n",
        "    break\n",
        "\n",
        "  # input and target are separated by tab\n",
        "  if '\\t' not in line:\n",
        "    continue\n",
        "\n",
        "  # split up the input and translation\n",
        "  line = line.rstrip().split('\\t')\n",
        "  input_text, translation = line[0], line[1]\n",
        "\n",
        "  if t < 10:\n",
        "    print(input_text, translation)\n",
        "\n",
        "  # make the target input and output\n",
        "  # recall we'll be using teacher forcing\n",
        "  target_text = translation + ' <eos>'\n",
        "  target_text_input = '<sos> ' + translation\n",
        "\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  target_texts_inputs.append(target_text_input)\n",
        "print(\"num samples:\", len(input_texts))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go. Va !\n",
            "Hi. Salut !\n",
            "Hi. Salut.\n",
            "Run! Cours !\n",
            "Run! Courez !\n",
            "Who? Qui ?\n",
            "Wow! Ça alors !\n",
            "Fire! Au feu !\n",
            "Help! À l'aide !\n",
            "num samples: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiFuWeHWZBok",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "efd7ab88-d96d-48a6-c41e-974037d8123d"
      },
      "source": [
        "# tokenize the inputs\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer_inputs.fit_on_texts(input_texts)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
        "\n",
        "# get the word to index mapping for input language\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
        "\n",
        "# determine maximum length input sequence\n",
        "max_len_input = max(len(s) for s in input_sequences)\n",
        "\n",
        "# tokenize the outputs\n",
        "# don't filter out special characters\n",
        "# otherwise <sos> and <eos> won't appear\n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
        "\n",
        "# get the word to index mapping for output language\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
        "\n",
        "# store number of output words for later\n",
        "# remember to add 1 since indexing starts at 1\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "\n",
        "# determine maximum length output sequence\n",
        "max_len_target = max(len(s) for s in target_sequences)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2146 unique input tokens.\n",
            "Found 5754 unique output tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg4XJVAOZEfS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dffd2682-d829-400c-f589-fc9d279667b3"
      },
      "source": [
        "# pad the sequences\n",
        "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
        "print(\"encoder_data.shape:\", encoder_inputs.shape)\n",
        "print(\"encoder_data[0]:\", encoder_inputs[0])\n",
        "\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
        "print(\"decoder_data[0]:\", decoder_inputs[0])\n",
        "print(\"decoder_data.shape:\", decoder_inputs.shape)\n",
        "\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_data.shape: (10000, 5)\n",
            "encoder_data[0]: [ 0  0  0  0 15]\n",
            "decoder_data[0]: [ 2 57  4  0  0  0  0  0  0  0  0]\n",
            "decoder_data.shape: (10000, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeSC0cMjZMUa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "411e852e-62fd-414b-9f2a-029c524cd709"
      },
      "source": [
        "# store all the pre-trained word vectors\n",
        "print('Loading word vectors...')\n",
        "word2vec = {}\n",
        "with open(os.path.join('sample_data/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
        "  # is just a space-separated text file in the format:\n",
        "  # word vec[0] vec[1] vec[2] ...\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:], dtype='float32')\n",
        "    word2vec[word] = vec\n",
        "print('Found %s word vectors.' % len(word2vec))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word vectors...\n",
            "Found 22005 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "youltgQTZRFe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21676a8e-a0a3-433e-ede2-8d7a0f26df77"
      },
      "source": [
        "# prepare embedding matrix\n",
        "print('Filling pre-trained embeddings...')\n",
        "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word2idx_inputs.items():\n",
        "  if i < MAX_NUM_WORDS:\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all zeros.\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filling pre-trained embeddings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG8A0WBbZT2x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1e69660e-2311-4910-e59e-1013169b48b9"
      },
      "source": [
        "# create embedding layer\n",
        "embedding_layer = Embedding(\n",
        "  num_words,\n",
        "  EMBEDDING_DIM,\n",
        "  weights=[embedding_matrix],\n",
        "  input_length=max_len_input,\n",
        "  # trainable=True\n",
        ")\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JUs20B-ZXWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create targets, since we cannot use sparse\n",
        "# categorical cross entropy when we have sequences\n",
        "decoder_targets_one_hot = np.zeros(\n",
        "  (\n",
        "    len(input_texts),\n",
        "    max_len_target,\n",
        "    num_words_output\n",
        "  ),\n",
        "  dtype='float32'\n",
        ")\n",
        "\n",
        "# assign the values\n",
        "for i, d in enumerate(decoder_targets):\n",
        "  for t, word in enumerate(d):\n",
        "    decoder_targets_one_hot[i, t, word] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52fozQAoZfTt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "db29798a-6a6b-4f68-cb84-5b6163067ea9"
      },
      "source": [
        "##### build the model #####\n",
        "\n",
        "# Set up the encoder - simple!\n",
        "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
        "x = embedding_layer(encoder_inputs_placeholder)\n",
        "encoder = Bidirectional(LSTM(\n",
        "  LATENT_DIM,\n",
        "  return_sequences=True,\n",
        "  # dropout=0.5 # dropout not available on gpu\n",
        "))\n",
        "encoder_outputs = encoder(x)\n",
        "\n",
        "\n",
        "# Set up the decoder - not so simple\n",
        "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
        "\n",
        "# this word embedding will not use pre-trained vectors\n",
        "# although you could\n",
        "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
        "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMwUXT1FZnG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######### Attention #########\n",
        "# Attention layers need to be global because\n",
        "# they will be repeated Ty times at the decoder\n",
        "attn_repeat_layer = RepeatVector(max_len_input)\n",
        "attn_concat_layer = Concatenate(axis=-1)\n",
        "attn_dense1 = Dense(10, activation='tanh')\n",
        "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
        "attn_dot = Dot(axes=1) # to perform the weighted sum of alpha[t] * h[t]\n",
        "\n",
        "def one_step_attention(h, st_1):\n",
        "  # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n",
        "  # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n",
        " \n",
        "  # copy s(t-1) Tx times\n",
        "  # now shape = (Tx, LATENT_DIM_DECODER)\n",
        "  st_1 = attn_repeat_layer(st_1)\n",
        "\n",
        "  # Concatenate all h(t)'s with s(t-1)\n",
        "  # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n",
        "  x = attn_concat_layer([h, st_1])\n",
        "\n",
        "  # Neural net first layer\n",
        "  x = attn_dense1(x)\n",
        "\n",
        "  # Neural net second layer with special softmax over time\n",
        "  alphas = attn_dense2(x)\n",
        "\n",
        "  # \"Dot\" the alphas and the h's\n",
        "  # Remember a.dot(b) = sum over a[t] * b[t]\n",
        "  context = attn_dot([alphas, h])\n",
        "\n",
        "  return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdtLL1CZZrw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the rest of the decoder (after attention)\n",
        "decoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True)\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "\n",
        "initial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\n",
        "initial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\n",
        "context_last_word_concat_layer = Concatenate(axis=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llDzES5yaS06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unlike previous seq2seq, we cannot get the output\n",
        "# all in one step\n",
        "# Instead we need to do Ty steps\n",
        "# And in each of those steps, we need to consider\n",
        "# all Tx h's\n",
        "\n",
        "# s, c will be re-assigned in each iteration of the loop\n",
        "s = initial_s\n",
        "c = initial_c\n",
        "\n",
        "# collect outputs in a list at first\n",
        "outputs = []\n",
        "for t in range(max_len_target): # Ty times\n",
        "  # get the context using attention\n",
        "  context = one_step_attention(encoder_outputs, s)\n",
        "\n",
        "  # we need a different layer for each time step\n",
        "  selector = Lambda(lambda x: x[:, t:t+1])\n",
        "  xt = selector(decoder_inputs_x)\n",
        "  \n",
        "  # combine \n",
        "  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
        "\n",
        "  # pass the combined [context, last word] into the LSTM\n",
        "  # along with [s, c]\n",
        "  # get the new [s, c] and output\n",
        "  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
        "\n",
        "  # final dense layer to get next word prediction\n",
        "  decoder_outputs = decoder_dense(o)\n",
        "  outputs.append(decoder_outputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyGrTJLWaXxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 'outputs' is now a list of length Ty\n",
        "# each element is of shape (batch size, output vocab size)\n",
        "# therefore if we simply stack all the outputs into 1 tensor\n",
        "# it would be of shape T x N x D\n",
        "# we would like it to be of shape N x T x D\n",
        "\n",
        "def stack_and_transpose(x):\n",
        "  # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
        "  x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n",
        "  x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n",
        "  return x\n",
        "\n",
        "# make it a layer\n",
        "stacker = Lambda(stack_and_transpose)\n",
        "outputs = stacker(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vJtE_Iiabo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the model\n",
        "model = Model(\n",
        "  inputs=[\n",
        "    encoder_inputs_placeholder,\n",
        "    decoder_inputs_placeholder,\n",
        "    initial_s, \n",
        "    initial_c,\n",
        "  ],\n",
        "  outputs=outputs\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNh-RJBkd3Np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "  # both are of shape N x T x K\n",
        "  mask = K.cast(y_true > 0, dtype='float32')\n",
        "  out = mask * y_true * K.log(y_pred)\n",
        "  return -K.sum(out) / K.sum(mask)\n",
        "\n",
        "\n",
        "def acc(y_true, y_pred):\n",
        "  # both are of shape N x T x K\n",
        "  targ = K.argmax(y_true, axis=-1)\n",
        "  pred = K.argmax(y_pred, axis=-1)\n",
        "  correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
        "\n",
        "  # 0 is padding, don't include those\n",
        "  mask = K.cast(K.greater(targ, 0), dtype='float32')\n",
        "  n_correct = K.sum(mask * correct)\n",
        "  n_total = K.sum(mask)\n",
        "  return n_correct / n_total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdg7YrHOhIGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='adam', loss=custom_loss, metrics=[acc])\n",
        "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# train the model\n",
        "z = np.zeros((len(encoder_inputs), LATENT_DIM_DECODER)) # initial [s, c]\n",
        "r = model.fit(\n",
        "  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=0.2\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# plot some data\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# accuracies\n",
        "plt.plot(r.history['acc'], label='acc')\n",
        "plt.plot(r.history['val_acc'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrHVfTLA_t9I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e1f8b1a3-b2bf-44f4-f1a1-89589de817df"
      },
      "source": [
        "##### Make predictions #####\n",
        "# As with the poetry example, we need to create another model\n",
        "# that can take in the RNN state and previous word as input\n",
        "# and accept a T=1 sequence.\n",
        "\n",
        "# The encoder will be stand-alone\n",
        "# From this we will get our initial decoder hidden state\n",
        "# i.e. h(1), ..., h(Tx)\n",
        "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n",
        "\n",
        "# next we define a T=1 decoder model\n",
        "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
        "\n",
        "# no need to loop over attention steps this time because there is only one step\n",
        "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
        "\n",
        "# combine context with last word\n",
        "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# lstm and final dense\n",
        "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
        "decoder_outputs = decoder_dense(o)\n",
        "\n",
        "\n",
        "# note: we don't really need the final stack and tranpose\n",
        "# because there's only 1 output\n",
        "# it is already of size N x D\n",
        "# no need to make it 1 x N x D --> N x 1 x D\n",
        "\n",
        "\n",
        "\n",
        "# create the model object\n",
        "decoder_model = Model(\n",
        "  inputs=[\n",
        "    decoder_inputs_single,\n",
        "    encoder_outputs_as_input,\n",
        "    initial_s, \n",
        "    initial_c\n",
        "  ],\n",
        "  outputs=[decoder_outputs, s, c]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# map indexes back into real words\n",
        "# so we can view the results\n",
        "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "  # Encode the input as state vectors.\n",
        "  enc_out = encoder_model.predict(input_seq)\n",
        "\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, 1))\n",
        "  \n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  # NOTE: tokenizer lower-cases all words\n",
        "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "  # if we get this we break\n",
        "  eos = word2idx_outputs['<eos>']\n",
        "\n",
        "\n",
        "  # [s, c] will be updated in each loop iteration\n",
        "  s = np.zeros((1, LATENT_DIM_DECODER))\n",
        "  c = np.zeros((1, LATENT_DIM_DECODER))\n",
        "\n",
        "\n",
        "  # Create the translation\n",
        "  output_sentence = []\n",
        "  for _ in range(max_len_target):\n",
        "    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
        "        \n",
        "\n",
        "    # Get next word\n",
        "    idx = np.argmax(o.flatten())\n",
        "\n",
        "    # End sentence of EOS\n",
        "    if eos == idx:\n",
        "      break\n",
        "\n",
        "    word = ''\n",
        "    if idx > 0:\n",
        "      word = idx2word_trans[idx]\n",
        "      output_sentence.append(word)\n",
        "\n",
        "    # Update the decoder input\n",
        "    # which is just the word just generated\n",
        "    target_seq[0, 0] = idx\n",
        "\n",
        "  return ' '.join(output_sentence)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "  # Do some test translations\n",
        "  i = np.random.choice(len(input_texts))\n",
        "  input_seq = encoder_inputs[i:i+1]\n",
        "  translation = decode_sequence(input_seq)\n",
        "  print('-')\n",
        "  print('Input sentence:', input_texts[i])\n",
        "  print('Predicted translation:', translation)\n",
        "  print('Actual translation:', target_texts[i])\n",
        "\n",
        "  ans = input(\"Continue? [Y/n]\")\n",
        "  if ans and ans.lower().startswith('n'):\n",
        "    break\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 2.9925 - acc: 0.0291 - val_loss: 2.5722 - val_acc: 0.1102\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 7s 894us/step - loss: 2.1165 - acc: 0.2537 - val_loss: 2.3679 - val_acc: 0.2512\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 7s 903us/step - loss: 1.9399 - acc: 0.2950 - val_loss: 2.2975 - val_acc: 0.2615\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 7s 905us/step - loss: 1.8511 - acc: 0.3016 - val_loss: 2.2585 - val_acc: 0.2643\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 7s 894us/step - loss: 1.7942 - acc: 0.3199 - val_loss: 2.2396 - val_acc: 0.2764\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 7s 891us/step - loss: 1.7475 - acc: 0.3368 - val_loss: 2.2016 - val_acc: 0.2780\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 7s 890us/step - loss: 1.6991 - acc: 0.3507 - val_loss: 2.1628 - val_acc: 0.2960\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 7s 874us/step - loss: 1.6351 - acc: 0.3699 - val_loss: 2.0937 - val_acc: 0.3042\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 7s 894us/step - loss: 1.5533 - acc: 0.3847 - val_loss: 2.0318 - val_acc: 0.3171\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 7s 891us/step - loss: 1.4647 - acc: 0.4070 - val_loss: 1.9656 - val_acc: 0.3427\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 7s 889us/step - loss: 1.3727 - acc: 0.4357 - val_loss: 1.8955 - val_acc: 0.3551\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 7s 904us/step - loss: 1.2839 - acc: 0.4600 - val_loss: 1.8477 - val_acc: 0.3695\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 7s 889us/step - loss: 1.2025 - acc: 0.4850 - val_loss: 1.7942 - val_acc: 0.3957\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 7s 887us/step - loss: 1.1311 - acc: 0.5085 - val_loss: 1.7816 - val_acc: 0.4143\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 7s 904us/step - loss: 1.0648 - acc: 0.5288 - val_loss: 1.7599 - val_acc: 0.4270\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 7s 880us/step - loss: 1.0039 - acc: 0.5447 - val_loss: 1.7413 - val_acc: 0.4398\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 7s 896us/step - loss: 0.9475 - acc: 0.5581 - val_loss: 1.7343 - val_acc: 0.4459\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 7s 882us/step - loss: 0.8941 - acc: 0.5696 - val_loss: 1.7242 - val_acc: 0.4529\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 7s 902us/step - loss: 0.8421 - acc: 0.5823 - val_loss: 1.7235 - val_acc: 0.4523\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 7s 888us/step - loss: 0.7931 - acc: 0.5975 - val_loss: 1.7115 - val_acc: 0.4590\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 7s 864us/step - loss: 0.7431 - acc: 0.6124 - val_loss: 1.7119 - val_acc: 0.4601\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 7s 898us/step - loss: 0.6951 - acc: 0.6261 - val_loss: 1.7069 - val_acc: 0.4625\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 7s 892us/step - loss: 0.6480 - acc: 0.6422 - val_loss: 1.6959 - val_acc: 0.4685\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 7s 878us/step - loss: 0.6030 - acc: 0.6599 - val_loss: 1.6901 - val_acc: 0.4754\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 7s 882us/step - loss: 0.5571 - acc: 0.6785 - val_loss: 1.6893 - val_acc: 0.4779\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 7s 883us/step - loss: 0.5150 - acc: 0.6973 - val_loss: 1.6951 - val_acc: 0.4763\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 7s 870us/step - loss: 0.4757 - acc: 0.7164 - val_loss: 1.6939 - val_acc: 0.4759\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 7s 878us/step - loss: 0.4378 - acc: 0.7352 - val_loss: 1.6946 - val_acc: 0.4807\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 7s 897us/step - loss: 0.4030 - acc: 0.7530 - val_loss: 1.6971 - val_acc: 0.4793\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 7s 881us/step - loss: 0.3708 - acc: 0.7682 - val_loss: 1.7047 - val_acc: 0.4812\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 7s 878us/step - loss: 0.3412 - acc: 0.7839 - val_loss: 1.7145 - val_acc: 0.4835\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 7s 871us/step - loss: 0.3145 - acc: 0.7951 - val_loss: 1.7009 - val_acc: 0.4851\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 7s 899us/step - loss: 0.2904 - acc: 0.8069 - val_loss: 1.7149 - val_acc: 0.4803\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 7s 873us/step - loss: 0.2682 - acc: 0.8164 - val_loss: 1.7107 - val_acc: 0.4856\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 7s 882us/step - loss: 0.2492 - acc: 0.8229 - val_loss: 1.7159 - val_acc: 0.4855\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 7s 879us/step - loss: 0.2323 - acc: 0.8299 - val_loss: 1.7208 - val_acc: 0.4829\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 7s 874us/step - loss: 0.2175 - acc: 0.8346 - val_loss: 1.7212 - val_acc: 0.4842\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 7s 895us/step - loss: 0.2037 - acc: 0.8394 - val_loss: 1.7272 - val_acc: 0.4841\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 7s 924us/step - loss: 0.1922 - acc: 0.8434 - val_loss: 1.7305 - val_acc: 0.4880\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 7s 894us/step - loss: 0.1817 - acc: 0.8452 - val_loss: 1.7412 - val_acc: 0.4898\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 7s 888us/step - loss: 0.1723 - acc: 0.8487 - val_loss: 1.7487 - val_acc: 0.4885\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 7s 881us/step - loss: 0.1645 - acc: 0.8502 - val_loss: 1.7503 - val_acc: 0.4899\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 7s 892us/step - loss: 0.1571 - acc: 0.8525 - val_loss: 1.7519 - val_acc: 0.4911\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 7s 874us/step - loss: 0.1511 - acc: 0.8535 - val_loss: 1.7568 - val_acc: 0.4896\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 7s 888us/step - loss: 0.1454 - acc: 0.8566 - val_loss: 1.7701 - val_acc: 0.4880\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 7s 901us/step - loss: 0.1405 - acc: 0.8557 - val_loss: 1.7726 - val_acc: 0.4911\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 7s 877us/step - loss: 0.1365 - acc: 0.8577 - val_loss: 1.7803 - val_acc: 0.4896\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 7s 896us/step - loss: 0.1322 - acc: 0.8601 - val_loss: 1.7806 - val_acc: 0.4917\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 7s 883us/step - loss: 0.1289 - acc: 0.8586 - val_loss: 1.7909 - val_acc: 0.4868\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 7s 901us/step - loss: 0.1257 - acc: 0.8592 - val_loss: 1.7872 - val_acc: 0.4907\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 7s 903us/step - loss: 0.1235 - acc: 0.8608 - val_loss: 1.7941 - val_acc: 0.4910\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 7s 887us/step - loss: 0.1207 - acc: 0.8603 - val_loss: 1.8183 - val_acc: 0.4877\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 7s 883us/step - loss: 0.1177 - acc: 0.8620 - val_loss: 1.8166 - val_acc: 0.4859\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 7s 896us/step - loss: 0.1151 - acc: 0.8638 - val_loss: 1.8238 - val_acc: 0.4858\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 7s 875us/step - loss: 0.1140 - acc: 0.8623 - val_loss: 1.8243 - val_acc: 0.4905\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 7s 905us/step - loss: 0.1118 - acc: 0.8621 - val_loss: 1.8267 - val_acc: 0.4925\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 7s 886us/step - loss: 0.1103 - acc: 0.8643 - val_loss: 1.8329 - val_acc: 0.4885\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 7s 872us/step - loss: 0.1087 - acc: 0.8639 - val_loss: 1.8354 - val_acc: 0.4862\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 7s 902us/step - loss: 0.1073 - acc: 0.8629 - val_loss: 1.8349 - val_acc: 0.4867\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 7s 883us/step - loss: 0.1068 - acc: 0.8618 - val_loss: 1.8597 - val_acc: 0.4906\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 7s 886us/step - loss: 0.1052 - acc: 0.8635 - val_loss: 1.8468 - val_acc: 0.4903\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 7s 878us/step - loss: 0.1041 - acc: 0.8634 - val_loss: 1.8617 - val_acc: 0.4887\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 7s 889us/step - loss: 0.1031 - acc: 0.8640 - val_loss: 1.8649 - val_acc: 0.4885\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 7s 890us/step - loss: 0.1021 - acc: 0.8641 - val_loss: 1.8614 - val_acc: 0.4898\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 7s 887us/step - loss: 0.1010 - acc: 0.8657 - val_loss: 1.8794 - val_acc: 0.4870\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 7s 885us/step - loss: 0.1005 - acc: 0.8637 - val_loss: 1.8843 - val_acc: 0.4879\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 7s 894us/step - loss: 0.0997 - acc: 0.8639 - val_loss: 1.8783 - val_acc: 0.4886\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 7s 870us/step - loss: 0.0991 - acc: 0.8631 - val_loss: 1.9011 - val_acc: 0.4853\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 7s 879us/step - loss: 0.0988 - acc: 0.8643 - val_loss: 1.9085 - val_acc: 0.4839\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 7s 873us/step - loss: 0.0984 - acc: 0.8645 - val_loss: 1.8948 - val_acc: 0.4858\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 7s 885us/step - loss: 0.0981 - acc: 0.8644 - val_loss: 1.9050 - val_acc: 0.4844\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 7s 889us/step - loss: 0.0975 - acc: 0.8637 - val_loss: 1.9132 - val_acc: 0.4824\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 7s 870us/step - loss: 0.0964 - acc: 0.8635 - val_loss: 1.9306 - val_acc: 0.4829\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 7s 898us/step - loss: 0.0957 - acc: 0.8634 - val_loss: 1.9205 - val_acc: 0.4881\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 7s 899us/step - loss: 0.0950 - acc: 0.8638 - val_loss: 1.9214 - val_acc: 0.4830\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 7s 887us/step - loss: 0.0943 - acc: 0.8633 - val_loss: 1.9323 - val_acc: 0.4842\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 7s 903us/step - loss: 0.0945 - acc: 0.8645 - val_loss: 1.9288 - val_acc: 0.4830\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 7s 872us/step - loss: 0.0939 - acc: 0.8639 - val_loss: 1.9321 - val_acc: 0.4842\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 7s 872us/step - loss: 0.0934 - acc: 0.8655 - val_loss: 1.9331 - val_acc: 0.4826\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 7s 908us/step - loss: 0.0930 - acc: 0.8640 - val_loss: 1.9448 - val_acc: 0.4818\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 7s 873us/step - loss: 0.0929 - acc: 0.8641 - val_loss: 1.9480 - val_acc: 0.4829\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 7s 884us/step - loss: 0.0927 - acc: 0.8648 - val_loss: 1.9506 - val_acc: 0.4827\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 7s 906us/step - loss: 0.0924 - acc: 0.8653 - val_loss: 1.9618 - val_acc: 0.4836\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 7s 895us/step - loss: 0.0920 - acc: 0.8637 - val_loss: 1.9569 - val_acc: 0.4833\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 7s 885us/step - loss: 0.0916 - acc: 0.8640 - val_loss: 1.9735 - val_acc: 0.4845\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 7s 878us/step - loss: 0.0915 - acc: 0.8621 - val_loss: 1.9780 - val_acc: 0.4815\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 7s 894us/step - loss: 0.0917 - acc: 0.8645 - val_loss: 1.9896 - val_acc: 0.4815\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 7s 901us/step - loss: 0.0924 - acc: 0.8645 - val_loss: 1.9783 - val_acc: 0.4821\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 7s 890us/step - loss: 0.0908 - acc: 0.8652 - val_loss: 1.9921 - val_acc: 0.4810\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 7s 879us/step - loss: 0.0906 - acc: 0.8640 - val_loss: 1.9979 - val_acc: 0.4813\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 7s 891us/step - loss: 0.0900 - acc: 0.8648 - val_loss: 1.9801 - val_acc: 0.4871\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 7s 874us/step - loss: 0.0903 - acc: 0.8646 - val_loss: 2.0037 - val_acc: 0.4793\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 7s 895us/step - loss: 0.0894 - acc: 0.8648 - val_loss: 2.0113 - val_acc: 0.4816\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 7s 887us/step - loss: 0.0895 - acc: 0.8643 - val_loss: 2.0128 - val_acc: 0.4813\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 7s 888us/step - loss: 0.0896 - acc: 0.8653 - val_loss: 2.0120 - val_acc: 0.4840\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 7s 888us/step - loss: 0.0894 - acc: 0.8653 - val_loss: 2.0077 - val_acc: 0.4846\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 7s 876us/step - loss: 0.0890 - acc: 0.8644 - val_loss: 2.0296 - val_acc: 0.4820\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 7s 888us/step - loss: 0.0895 - acc: 0.8636 - val_loss: 2.0262 - val_acc: 0.4812\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 7s 888us/step - loss: 0.0887 - acc: 0.8639 - val_loss: 2.0256 - val_acc: 0.4810\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 7s 880us/step - loss: 0.0899 - acc: 0.8621 - val_loss: 2.0420 - val_acc: 0.4825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCBEjJNCFECJGSKALIUSM+P9ubTdxrcV05AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: He hugged her.\n",
            "Predicted translation: il l'enlaça.\n",
            "Actual translation: Il l'enlaça. <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: It's garbage.\n",
            "Predicted translation: ce sont des conneries.\n",
            "Actual translation: C'est de la daube. <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: Tom cheats.\n",
            "Predicted translation: tom triche.\n",
            "Actual translation: Tom triche. <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: Drink it down.\n",
            "Predicted translation: bois-le !\n",
            "Actual translation: Bois-le ! <eos>\n",
            "Continue? [Y/n]n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T5Z0CrRhTgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}