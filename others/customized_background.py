# -*- coding: utf-8 -*-
"""Customized_Background

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s-sPlVal8AOqqhDiNwLOyXWdB9XiVJuo

## **Customizing the background of images**

I am using DeepLabV3 for achieving semantic segmentation and alpha blending to blend a new background with the original image.
DeepLab is a state-of-the-art deep learninf model for semantic image segmentation. It is a state-of-art deep learning model for semantic image segmentation, where the goal is to assign semantic labels to each pixel within a specific class in the input image. DeepLabv1: use atrous convolution to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks that implore Atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views.

# Import Libraries
"""

from google.colab import files
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
import torch
import torchvision.transforms as T
from torchvision import models
from PIL import Image
import cv2
import numpy as np
import glob

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

"""# Load the image"""

from google.colab import drive
drive.mount('/content/drive')

img1 = cv2.imread("/content/drive/My Drive/dayo.jpeg")
background1 = cv2.imread("/content/drive/My Drive/Petra.jpg")
background2 = cv2.imread("/content/drive/My Drive/Chichen_Itza.jpg")
background3 = cv2.imread("/content/drive/My Drive/Machu-Picchu.jpg")
background4 = cv2.imread("/content/drive/My Drive/Colosseum-Rome-Italy.jpg")
background5 = cv2.imread("/content/drive/My Drive/statue-Christ-the-Redeemer-Rio-de-Janeiro.jpg")
background6 = cv2.imread("/content/drive/My Drive/Taj-Mahal-Agra-India.jpg")
background7 = cv2.imread("/content/drive/My Drive/Peat-bog.jpg")

# show my fine face(the image you wanna blend its backgroung)
plt.imshow(img1);
plt.show()

plt.imshow(background1); plt.show()

#show the backgrounds you wanna blend image to 
plt.subplot(7, 4, 1, label = "Petra") 
plt.imshow(background1) 
  
# ncols stays as 1  
plt.subplot(7, 4, 2) 
plt.imshow(background2) 

# ncols stays as 2  
plt.subplot(7, 4, 2) 
plt.imshow(background3)  

# ncols stays as 3 
plt.subplot(7, 4, 3) 
plt.imshow(background3)

# ncols stays as 4  
plt.subplot(7, 4, 4) 
plt.imshow(background4)  

# ncols stays as 1  
plt.subplot(7, 4, 5) 
plt.imshow(background5)  

# ncols stays as 1  
plt.subplot(7, 4, 6) 
plt.imshow(background6)  

# ncols stays as 1  
plt.subplot(7, 4, 7) 
plt.imshow(background7)

"""# Decode Output
The following to convert from 2D image to RGB image, mapping each label is map to a color
"""

def image_to_rgb(img, nc=21):
  '''
  Perform segmantic sementation on the image 
  @params: 
  img: numpy.ndarray. 2D array of image pixel
  nc: int. The number of class of each object we have in an image
  @return 
  rgb:numpy.ndarray. The rgb equivalence of image semantic segmentation
  '''
  label_colors = np.array([(0, 0, 0),  # 0=background
               # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle
               (128, 0, 0), (0, 128, 0), (255, 255, 255), (0, 0, 128), (128, 0, 128),
               # 6=bus, 7=car, 8=cat, 9=chair, 10=cow
               (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),
               # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person
               (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (255, 255, 255),
               # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor
               (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)])

  r = np.zeros_like(image).astype(np.uint8)
  g = np.zeros_like(image).astype(np.uint8)
  b = np.zeros_like(image).astype(np.uint8)
  
  for l in range(0, nc):
    idx = image == l
    r[idx] = label_colors[l, 0]
    g[idx] = label_colors[l, 1]
    b[idx] = label_colors[l, 2]
    
  rgb = np.stack([r, g, b], axis=2)
  return rgb

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
import timeit

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print(
      '\n\nThis error most likely means that this notebook is not '
      'configured to use a GPU.  Change this in Notebook Settings via the '
      'command palette (cmd/ctrl-shift-P) or the Edit menu.\n\n')
  raise SystemError('GPU device not found')

"""# Image Preprocessing

Preprocess the image to the required format of the model
"""

def segment(net, path, show_orig=True, dev='cuda'):
  with tf.device('/device:GPU:0'):
    '''
    Parameters: 
    net: Neural Network. 
    path: String. Directory of the image
    show_original: Boolean.If you want to show the original image 
    dev: GPU. Specify the GPU you wanna use
    Return:
    rgb: FloatTensor
    '''
    img = Image.open(path)
    #if show_orig: plt.imshow(path); plt.show()

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # convert data to a normalized torch.FloatTensor
    transform = T.Compose([#T.Resize(640), #Comment the Resize and CenterCrop for better inference results
                         #T.CenterCrop(224), 
                         T.ToTensor(), 
                         T.Normalize(mean=[0.485, 0.456, 0.406], #normalize in accordance with imagenet specs
                                     std = [0.229, 0.224, 0.225])])
    input = transform(img).unsqueeze(0) #unsqueeze(0) is used to add extra layer of dimensionality to the image 
    input = input  #move input to device 
    out = net(input)['out'] #run model on the image
    om = torch.argmax(out.squeeze(), dim=0).detach.cpu.numpy() #take hardmax
    rgb = image_to_rgb(om)
    plt.imshow(rgb); plt.axis('off'); plt.show()
    cv2.imwrite('mask2.png' , rgb)
    files.download('mask2.png')
    return rgb

from torchvision import transforms
input_image = Image.open( "/content/drive/My Drive/dayo.jpeg")
preprocess = T.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

input_tensor = preprocess(input_image)
input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model

# move the input and model to GPU for speed if available
if torch.cuda.is_available():
    input_batch = input_batch.to('cuda')
    model.to('cuda')

with torch.no_grad():
    output = model(input_batch)['out'][0]
output_predictions = output.argmax(0)

output_predictions.shape

# Define the helper function
def decode_segmap(image, nc=21):
  
  label_colors = np.array([(0, 0, 0),  # 0=background
               # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle
               (128, 0, 0), (0, 128, 0), (255, 255, 255), (0, 0, 128), (128, 0, 128),
               # 6=bus, 7=car, 8=cat, 9=chair, 10=cow
               (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),
               # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person
               (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (255, 255, 255),
               # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor
               (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)])

  r = np.zeros_like(image).astype(np.uint8)
  g = np.zeros_like(image).astype(np.uint8)
  b = np.zeros_like(image).astype(np.uint8)
  
  for l in range(0, nc):
    idx = image == l
    r[idx] = label_colors[l, 0]
    g[idx] = label_colors[l, 1]
    b[idx] = label_colors[l, 2]
    
  rgb = np.stack([r, g, b], axis=2)
  return rgb

rgb = decode_segmap(output_predictions.byte().cpu().numpy())
rgb.shape

import torch
model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True)
model.eval()

# Download the pre- trained model
dlab = models.segmentation.deeplabv3_resnet101(pretrained=1).eval()

rgb=segment(model, "/content/drive/My Drive/dayo.jpeg")

# create a color pallette, selecting a color for each class
palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])
colors = torch.as_tensor([i for i in range(21)])[:, None] * palette
colors = (colors % 255).numpy().astype("uint8")

# plot the semantic segmentation predictions of 21 classes in each color
r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)
r.putpalette(colors)

import matplotlib.pyplot as plt
plt.imshow(r)

# to be use if there are multiple image in background to make target for ground
rgb[rgb!=255]=0

"""# Change the background with alpha blending"""

#alpha blending to customize the background of the image

def alpha_blending(image_path, background_path, rgb, result_path):
  '''
  Use alpha blending to create a customized background
  Parameters:
  image: String. The path to the image to be used as foreground
  background: String: The path to the image to be used as background
  rgb: Segmented image.
  Return
  completed: String. to indicate completion 
  '''
  # Read the images
  foreground = cv2.imread(image_path)
  background = cv2.imread(background_path, cv2.IMREAD_COLOR)
  background = cv2.resize(background, (rgb.shape[1],rgb.shape[0]), interpolation = cv2.INTER_AREA)
  
  alpha = rgb
  
  # Convert uint8 to float
  foreground = foreground.astype(float)
  background = background.astype(float)
 
  # Normalize the alpha mask to keep intensity between 0 and 1
  alpha = alpha.astype(float)/255
 
  # Multiply the foreground with the alpha matte
  foreground = cv2.multiply(alpha, foreground)
 
  # Multiply the background with ( 1 - alpha )
  background = cv2.multiply(1.0 - alpha, background)
 
  # Add the masked foreground and background.
  outImage = cv2.add(foreground, background)

  # Display image
  plt.imshow(outImage); plt.show()
  cv2.waitKey(0)
  
  cv2.imwrite(result_path , outImage)
  files.download(result_path)
  completed = "Done"
  return completed

#test case one
alpha_blending(image_path, background_path, rgb, result_path)

rgb.shape

background_path_0 = "/content/drive/My Drive/Petra.jpg"
background_path_1 = "/content/drive/My Drive/Chichen_Itza.jpg"
background_path_2 = "/content/drive/My Drive/Machu-Picchu.jpg"
background_path_3 = "/content/drive/My Drive/Colosseum-Rome-Italy.jpg"
background_path_4 = "/content/drive/My Drive/statue-Christ-the-Redeemer-Rio-de-Janeiro.jpg"
background_path_5 = "/content/drive/My Drive/Taj-Mahal-Agra-India.jpg"
background_path_6 = "/content/drive/My Drive/Peat-bog.jpg"

image_path = "/content/drive/My Drive/dayo.jpeg"
alpha_blending(image_path, background_path_0, rgb, 'custom/dayo_petra.png')

#test 2
image_path = "/content/drive/My Drive/dayo.jpeg"
alpha_blending(image_path, background_path_1, rgb, 'custom/dayo_sw1.png')

#test 3
image_path = "/content/drive/My Drive/dayo.jpeg"
alpha_blending(image_path, background_path_2, rgb, 'custom/dayo_sw2.png')

#test4
image_path = "/content/drive/My Drive/dayo.jpeg"
alpha_blending(image_path, background_path_3, rgb, 'custom/dayo_sw3.png')

#test5
image_path = "/content/drive/My Drive/dayo.jpeg"
alpha_blending(image_path, background_path_4, rgb, 'custom/dayo_sw4.png')

#test6
image_path = "/content/drive/My Drive/dayo.jpeg"
alpha_blending(image_path, background_path_5, rgb, 'custom/dayo_sw5.png')

#test7
image_path = "/content/drive/My Drive/dayo.jpeg"
alpha_blending(image_path, background_path_6, rgb, 'custom/dayo_sw6.png')

"""# Create Transition Video"""

img_array = []
for filename in glob.glob('custom/*.png'):
    img = cv2.imread(filename)
    height, width, layers = img.shape
    size = (width,height)
    img_array.append(img)

len(img_array)

out = cv2.VideoWriter('project1.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)
 
count = 0
for count in range(6): 
  for i in np.linspace(0, 1, 10):
    alpha = i
    beta = 1 -i
    blend_image = cv2.addWeighted(img_array[count],alpha, img_array[count+1], beta, 0)
    out.write(blend_image)
out.release()

files.download('project1.avi')

cv2_imshow('dayo_sw1.png')

